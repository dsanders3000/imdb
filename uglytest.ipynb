{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# butchered ugly test version\n",
    "# imdb movie review sentiment analysis with lstm\n",
    "\n",
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout\n",
    "from keras.layers import LSTM, CuDNNLSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "from keras import __version__\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "assert(LV(__version__) >= LV(\"2.0.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB data set\n",
    "\n",
    "load imdb dataset\n",
    "\n",
    "50000 movie reviews\n",
    "\n",
    "25000 training\n",
    "\n",
    "25000 testing\n",
    "\n",
    "half are postive (1) half are negative (0)\n",
    "\n",
    "dataset is in preprocessed form and words have been replaced by integer\n",
    "\n",
    "words indices start at 3, 1 to mark start of review, 2 represents out of vocab words, 0 used to pad\n",
    "\n",
    "some weird security issue which requires TRUE pickle- no idea what this is but it is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of most-frequent words to use\n",
    "nb_words = 10000\n",
    "# cut texts after this number of words\n",
    "maxlen = 80\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "\n",
    "#weird security thing below\n",
    "old = np.load\n",
    "np.load = lambda *a,**k: old(*a,**k,allow_pickle=True)\n",
    "\n",
    "from keras.datasets import reuters\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=nb_words)\n",
    "\n",
    "np.load = old\n",
    "del(old)\n",
    "#weird security thing above\n",
    "\n",
    "print('x_train:', x_train.shape)\n",
    "print('x_test:', x_test.shape)\n",
    "print()\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first movie review in the training set in integer form\n",
    "\n",
    "this can be translated into words (in other notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First review in the training set:\\n\", x_train[0], \"length:\", len(x_train[0]), \"class:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation\n",
    "\n",
    "make rnn:\n",
    "\n",
    "input reviews\n",
    "\n",
    "first layer is embedding layer- each word is taught a value and is connected\n",
    "\n",
    "second layer is the dropout layer which always preceeds the lstm\n",
    "\n",
    "third layer is lstm layer with 32 hidden units\n",
    "\n",
    "dense layer next with sigmoid activation for classification\n",
    "\n",
    "compile() model using *binary crossentropy* as the loss function and *RMSprop* as the optimizer\n",
    "\n",
    "optimiser is not ideal- ADAM is preferred\n",
    "\n",
    "get some error in colab- don't know why yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters:\n",
    "embedding_dims = 50\n",
    "lstm_units = 32\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(nb_words,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(lstm_units))\n",
    "# if running on a GPU:\n",
    "#model.add(CuDNNLSTM(lstm_units))\n",
    "\n",
    "# if multiple LSTM laters:\n",
    "#model.add(LSTM(lstm_units, return_sequences=True))\n",
    "#model.add(LSTM(lstm_units))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "train lstm on cpu\n",
    "\n",
    "use `model.add(CuDNNLSTM(lstm_units))` instead of `model.add(LSTM(lstm_units))` to run on gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 5\n",
    "validation_split = 0.2\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=128,\n",
    "          epochs=epochs, \n",
    "          validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_loss'], label='validation')\n",
    "plt.title('loss')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['acc'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_acc'], label='validation')\n",
    "plt.title('accuracy')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*To run in windows use [this](https://colab.research.google.com/github/dsanders3000/imdb/blob/master/uglytest.ipynb).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
